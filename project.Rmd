---
title: "Practical Machine Learning: Prediction Assignment"
author: "Brooke"
date: "Thursday, June 11, 2015"
output: html_document
---
<p>
<p>
### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website <a href="http://groupware.les.inf.puc-rio.br/har">here</a> (see the section on the Weight Lifting Exercise Dataset). 


### Data 


The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from <a href="http://groupware.les.inf.puc-rio.br/har">Groupware@LES</a>. 

### Objectives:  

The goal of this project is to predict the manner in which the subjects did the exercise (**"classe"** variable in the training set). 


### Reading & Cleaning Data  

```{r}
train<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))  
test<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))  
```

To clean the data, we will need to determine columns/variables to be removed: "bookkeeping columns", zero covariates, and NA values. 

**Step 1: Finding "bookkeeping columns"** (look at the first 15 column names):  

```{r}
names(train)[1:15] 
```

Columns 1-7 are the "bookkeeping columns" (user name, time stamps, window number, etc). They will not be helpful in building our prediction model.

**Step 2: Find zero covariates**:  

We will use the function **nearZeroVar()** from the **caret** package. **saveMetrics=FALSE** (default setting) conveniently returns the positions of zero or near zero variance predictors.  

```{r message=FALSE}
library(caret)
nearZeroVar(train)  
```

We will now remove all columns we have found above:  

```{r}
index<-c(1:7, 14,17,26, 51:59, 75, 78, 79, 81, 82, 89, 92, 101, 127, 130, 131, 134, 137, 139, 142:150 )  
Train<-train[, -index]
```

**Step 3: NA values**  

Now let's find out the rate of NAs in the remaining columns:  

```{r}
naRate<-colSums(is.na(Train))/nrow(Train)  
naRate  
```

We can see that NA rate is either 0% (i.e., those predictors have no missing values) or 97-98% (i.e., those columns are mostly comprised of missing values). Naturally, we will keep the predictors with 0% NAs and remove the ones with 97-98% NAs.  

```{r}
Index<-which(naRate==0.00)  
Train<-Train[, Index]  
```

Check to make sure there are no missing values in the remaining predictors:  

```{r results='hide'}
colSums(is.na(Train)) # no missing values
```
```{r}
dim(Train)  
names(Train)  
```

Our clean Train dataset has 53 variables: 52 predictors and one outcome variable, **classe**. Before we proceed with the model training, we will clean the test set in the same exact manner we did the train set. Note that the test set has an extra variable **problem_id**, which we will remove as well (not a predictor -- another "bookkeeping column"):  

```{r}
Test<-test[, -index]  
Test<-Test[, Index] 
str(Test$problem_id) # making sure the variable is not of value
Test<-Test[, -53] # removing column problem_id
dim(Test) 
```

The test set now has the same 52 predictors as the training set. The last step in cleaning/preparing the data will be to "factorise" outcome variable **classe** in the Train set:  

```{r}
Train$classe<-as.factor(Train$classe)  
```

### Prediction Model

In order to estimate the test error (that is, the error when we apply the prediction model to the data that was not used to train the model, also known as out of sample error) and ultimately, determine how well a statistical learning method that we choose can be expected to perform on independent data, we need to perform cross validation. To do so, we will use the the validation set approach and split the clean training set into sub-training and validation sets, with the help of the **createDataPartition()** function from the **caret** package. 

**Step 1: Separate the Train data into training (60% of the Train data) and validation (40% of the Train data) sets, and remove the outcome variable from both resulting sets**:  

```{r}
set.seed(357)
inTrain<-createDataPartition(Train$classe, p=0.6, list=FALSE)  
training<-Train[inTrain, ]  
valid<-Train[-inTrain, ]  
data<-training[-53] 
newdata<-valid[-53]  
```
 
**Step 2: Fit a model**  

We will be predicting with Random Forest, which is one of the most accurate predicting algorithms. In order to speed things up, we will use Parallel Random Forest (which is quite faster than fitting Random Forest, see <a href="http://www.r-bloggers.com/introducing-parallelrandomforest-faster-leaner-parallelized/">this link</a> and <a href="http://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf">this link</a> for more details). 

We want to create a Random Forest model with a total of 1000  trees. Our computer has 2 cores in it, so we can split the number of trees to be constructed into 2 parts, 500 for each core. **combine** function that comes with **randomForest** package will combine the resulting Random Forest objects into one model.  


```{r message=FALSE}
set.seed(1379)
library(foreach)
library(randomForest)
library(doParallel)
```
```{r warning=FALSE}
RF<-foreach(ntree=rep(500,2), .combine=combine, .packages='randomForest') %dopar% randomForest(data, training$classe, ntree=ntree)  
RF  
```


Let's take a look the plot of important variables in our model:  

```{r fig.width=8, fig.height=7}
varImpPlot(RF, main="Random Forest model: Important Variables", col="blue", pch=19)
```

Out of a total of 52 predictors, only about 30 were the most important ones to train our prediction model.

**In sample error** (a.k.a. train error): 

Since we are using the data on which we trained the model, we expect this model to perform very well on the training dataset, with a high accuracy rate and low error rates:

```{r}
pred<-predict(RF, data)  
cm<-confusionMatrix(pred, training$classe)  
cm  
```

The **in sample error** is virtually non-existent: the accuracy rate for the training set is 100% -- with such high accuracy rate, there is a danger of possibly overfitting the training data. If that is indeed the case, we will expect **out of sample error** to be significantly higher, and the accuracy to be significantly lower. However, even if there is no overfitting, the **out of sample error** will always be higher than **in sample error**.

**Out of Sample Error (a.k.a. test error): Predictions & Confusion Matrix**    

```{r}
predRF<-predict(RF, newdata)  
cmRF<-confusionMatrix(predRF, valid$classe) 
cmRF  
```

As expected, the **out of sample error** rates (a.k.a. test error) are higher than the **in sample/train error** rates above, and the accuracy rate is lower. However, the accuracy rate is still very high, and so is Kappa statistic (which indicates how closely the instances classified by the machine learning classifier matched the true results), and error rates for each class seem to be extremely low (see Statistics by Class in the output above):  
```{r echo=FALSE}
paste("Model Accuracy:", "", round(cmRF$overall["Accuracy"], 4))
paste("Kappa Statistic:", "", round(cmRF$overall["Kappa"], 4))
```

We can certainly see that the reputation of Random Forest as one of the most accurate prediction algorithms is not unfounded.

**Independent Test set: Predictions**

We will now predict the results for the Test set and see how our machine learning method performs on independent data:  

```{r}
predictions<-predict(RF, Test)  
```

Using the code provided by the Coursera instructors, we will prepare the files with the answers:  

```{r eval=FALSE}
 pml_write_files = function(x){
+     n = length(x)
+     for(i in 1:n){
+         filename = paste0("problem_id_",i,".txt")
+         write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
+     }
+ }  

pml_write_files(predictions)  
```

These results have been submitted and deemed correct (with 100% accuracy), which supports the high accuracy rate of our model.