head(a)
head(ToothGrowth)
str(ToothGrowth)
ToothGrowth$dose<-as.numeric(ToothGrowth$dose)
str(ToothGrowth)
a<-separate(ToothGrowth, supp, c("OJ", "VC"))
head(a)
library(reshape2)
data(mtcars)
head(mtcars)
mtcars$carname<-rownames(mtcars)
carMelt<-melt(mtcars, id=c("carname", "gear", "cyl"), measure.vars=c("mpg", "hp"))
head(carMelt)
cylData<-dcast(carMelt, cyl~variable)
cylData
data(ToothGrowth)
str(ToothGrowth)
summary(ToothGrowth)
ggplot(data=ToothGrowth, aes(x=as.factor(dose), y=len, fill=supp)) +
geom_bar(stat="identity",) +
facet_grid(. ~ supp) +
xlab("Dose (in mg)") +
ylab("Mean Length of Odontoblasts") +
guides(fill=guide_legend(title="Supplement"))
library(ggplot2)
ggplot(data=ToothGrowth, aes(x=as.factor(dose), y=len, fill=supp)) +
geom_bar(stat="identity",) +
facet_grid(. ~ supp) +
xlab("Dose (in mg)") +
ylab("Mean Length of Odontoblasts") +
guides(fill=guide_legend(title="Supplement"))
head(ToothGrowth)
str(ToothGrowth$supp)
VC=ToothGrowth$supp[1:30]
VC
VC=ToothGrowth$len[1:30]
VC
OJ<-ToothGrowth$len[31:60]
ToothGrowth[27:33]
ToothGrowth[27:33, ]
?t.test
t.test(VC, OJ)
1mg<-subset(ToothGrowth, dose==2.0, select=len)
one_mg<-subset(ToothGrowth, dose==2.0, select=len)
one_mg
two_mg<-subset(ToothGrowth, dose==2.0, select=len)
one_mg<-subset(ToothGrowth, dose==1.0, select=len)
t.test(one_mg, two_mg)
t.test(one_mg, two_mg)$ht
t.test(one_mg, two_mg)$htest
t.test(one_mg, two_mg)
t.test(one_mg, two_mg)$two.sided
summary(ToothGrowth$len)
summary(ToothGrowth$dose)
summary(ToothGrowth$sup)
str(ToothGrowth)
t.test(one_mg, two_mg)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain<-createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training=segmentationOriginal[inTrain, ]
testing=segmentationOriginal[-inTrain, ]
set.seed(125)
modFit=train(Case~., method="rpart", data=training)
print(modFit$finalModel)
predict(modFit, newdata=testing)
modFit=train(Class~., method="rpart", data=training)
predict(modFit, newdata=testing)
str(segmentationOriginal)
training=subset(segmentationOriginal, Case=="Train", select=-1)
testing=subset(segmentationOriginal, Case=="Test", select=-1)
training=subset(segmentationOriginal, Case=="Train", select=-2)
testing=subset(segmentationOriginal, Case=="Test", select=-2)
str(testing)
set.seed(125)
modFit=train(Classs~., method="rpart", data=training)
modFit=train(Class~., method="rpart", data=training)
modFit
modFit$finalModel
rattle: :fancyRpartPlot(modFit$finalModel)
rattle::fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
library(rattle)
rattle::fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
library(rpart.plot)
rattle::fancyRpartPlot(modFit$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
str(olive)
tree=train(Area~., method="rpart", data=olive)
newdata=as.data.frame(t(colMeans(olive)))
predict(tree, newdata=newdata)
str(olive$Area)
unique.values(olive$Area)
uniqueValues(olive$Area)
unique(olive$Area)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
str(testSA)
model=train(chd~age+alcohol+obesity+tobacco+typea+ldl, method="glm", family="binomial", data=trainSA)
predictions=predict(model, newdata=testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(prediction=predictions)
missClass(values=testSA$chd, prediction=predictions)
missClass(values=trainSA$chd, prediction=predict(model, newdata=trainSA))
set.seed(13234)
model=train(chd~age+alcohol+obesity+tobacco+typea+ldl, method="glm", family="binomial", data=trainSA)
predictions=predict(model, newdata=testSA)
missClass(values=testSA$chd, prediction=predictions)
missClass(values=trainSA$chd, prediction=predict(model, newdata=trainSA))
?ElemStatLearn
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
vowel.train$y=as.factor(as.character(vowel.train$y))
str(vowel.train)
vowel.test$y=as.factor(as.character(vowel.test$y))
str(vowel.test)
set.seed(33833)
rf=train(y~., method="rf", prox=TRUE)
rf=train(y~., method="rf", prox=TRUE, data=vowel.train)
? varImp
varImp(rf, useModel=TRUE)
rf=train(y~., method="rf", importance=FALSE, data=vowel.train)
set.seed(33833)
rf=train(y~., method="rf", importance=FALSE, data=vowel.train)
varImp(rf, useModel=TRUE)
order(varImp(rf, useModel=TRUE))
set.seed(33833)
rf1=randomForest(y~., data=vowel.train)
varImp(rf1)
order(varImp(rf1))
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
b
order(varImp(rf))
varImp(rf)
order(varImp(rf1))
?trainControl
str(vowel.test)
set.seed(33833)
rf=train(y~., method="rf", prox=TRUE, data=vowel.train)
predictionrf=predict(rf, vowel.test)
confusionMatrix(predictionrf, vowel.test$y)
boo=train(y~., method="gbm", verbose=FALSE, data=vowel.train)
predictionBoo=predict(boo, vowel.test)
confusionMatrix(predictionBoo, vowel.test$y)
confusionMatrix(predictionBoo, vowel.test$y)["Accuracy"]
set.seed(33833)
rf=train(y~., method="rf", data=vowel.train)
boo=train(y~., method="gbm", data=vowel.train)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.train$y=as.factor(as.character(vowel.train$y))
vowel.test$y=as.factor(as.character(vowel.test$y))
str(vowel.train)
set.seed(33833)
rf=train(y~., method="rf", data=vowel.train)
boo=train(y~., method="gbm", data=vowel.train, verbose=FALSE)
predRF=predict(rf, vowel.test$y)
predRF=predict(rf, vowel.test)
predBoo=predict(boo, vowel.test)
CMrf=confusionMatrix(predRF, vowel.test$y)
CMboo=confusionMatrix(predboo, vowel.test$y)
CMboo=confusionMatrix(predBoo, vowel.test$y)
CMrf$overall["Accuracy"]
CMboo$overall["Accuracy"]
combined=data.frame(predRF==predBoo)
confusionMatrix(vowel.test$y[combined], predRF[combined])$overall['Accuracy']
combined=data.frame(predRF, predBoo, y=vowel.test$y)
fit=train(y~., data=combined)
predFit=predict(fit, vowel.test)
CMfit=confusionMatrix(predFit, vowel.test$y)
CMfit$overall["Accuracy"]
?train
fit=train(y~., data=combined, method="gam")
predFit=predict(fit, vowel.test)
CMfit=confusionMatrix(predFit, vowel.test$y)
CMfit$overall["Accuracy"]
it=train(y~., data=combined)
predFit=predict(fit, vowel.test)
CMfit=confusionMatrix(predFit, vowel.test$y)
CMfit$overall["Accuracy"]
fit=train(y~., data=combined)
predFit=predict(fit, vowel.test)
CMfit=confusionMatrix(predFit, vowel.test$y)
CMfit$overall["Accuracy"]
names(getModelInfo())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed( 62433)
RF=train(diagnosis~., data=training, method="rf")
BOO=train(diagnosis~., data=training, method="gbm", verbose=FALSE)
LDA=train(diagnosis~., data=training, method="lda")
PredRF=predict(RF, training)
PredRF=predict(RF, testing)
rm(PredRF)
PredRF=predict(RF, testing)
PredBOO=predict(BOO, testing)
PredLDA=predict(LDA, testing)
cmRF=confusionMatrix(PredRF, testing$diagnosis)
cmBOO=confusionMatrix(PredBOO, testing$diagnosis)
cmLDA=confusionMatrix(PredLDA, testing$diagnosis)
cmRF$overall["Accuracy"]
cmBOO$overall["Accuracy"]
cmLDA$overall["Accuracy"]
comb=data.frame(PredRF, PredBOO, PredLDA, diagnosis=testing$diagnosis)
fit=train(diagnosis~., data=comb, method="rf")
PredFit=predict(fit, testing)
cmFIT=confusionMatrix(PredFit, testing$diagnosis)
cmfit$overall["Accuracy"]
cmFIT$overall["Accuracy"]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
str(testing)
mod=train(CompressiveStrength~.,data=training, method="lasso")
?plot.enet
plot.enet(mod)
mod$finalModel
plot.enet(mod$finalModel, xvar=penalty)
plot.enet(mod$finalModel, xvar="penalty", use.color=TRUE)
temp=tempfile()
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv", temp)
data=read.csv(temp)
dat=data
rm(data)
library(lubridate)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
?bats
mod <- bats(tstrain)
fcast <- forecast.bats(mod, level=95, h=nrow(testing))
acc <- accuracy(fcast, testing$visitsTumblr)
count <- 0
for (i in 1:nrow(testing)) {
if (testing$visitsTumblr[i] > fcast$lower[i]) {
if(testing$visitsTumblr[i] < fcast$upper[i]) {
count <- count + 1}
}
}
count/nrow(testing)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library("e1071", lib.loc="~/R/win-library/3.2")
fit <- svm(CompressiveStrength ~., data=training)
pred=predict(fit, testing)
acc=accuracy(pred, testing$CompressiveStrength)
acc
fit1=train(CompressiveStrength~., data=training, method="svm")
library(MASS)
data(shuttle)
?shuttle
str(shuttle)
model=glm(use~wind, data=shuttle, family-"binomial")
model=glm(use~wind, data=shuttle, family="binomial")
summary(model)
exp(model$coeff)
model1=glm(use~wind+magn, data=shuttle, family="binomial")
exp(model1$coeff)
data(insectSprays)
data(InsectSprays)
head(InsectSprays)
str(InsectSprays)
mod=glm(count~spray, data=InsectSprays, family="poisson")
summary(mod)
mod=glm(count~spray-1, data=InsectSprays, family="poisson")
summary(mod)
2.67415/2.73003
exp(mod$coef[1])/exp(mod$coef[2])
exp(mod$coeff)
14.5000/15.3333
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knots=(0, length=1)
knots=c(0)
splineTerms<-sapply(knots,function(knot) (x>knot)*(x-knot))
xmat<-cbind(1,x,splineTerms)
fit<-lm(y~xmat-1)
yhat<-predict(fit)
summary(fit)$coef
fit1<-lm(y~xmat)
yhat1<-predict(fit1)
summary(fit1)$coef
(yhat[10]-yhat[6])/4
plot(x,y)
lines(x,yhat,col="red")
library(pryr)
ftype(mean)
install.packages("pryr")
library(pryr)
ftype(mean)
ftype(colSums)
ftype(lm)
ftype(dgamma)
?ftype
library(shiny)
server <- function(input, output) {}
shinyApp(ui = ui, server = server)
ui <- fluidPage()
server <- function(input, output) {}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput(inputId = "num",
label = "Choose a number",
value = 25, min = 1, max = 100),
plotOutput("hist")
)
server <- function(input, output) {
output$hist <- renderPlot({
hist(rnorm(input$num))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput(inputId = "num",
label = "Choose a number",
value = 25, min = 1, max = 100),
plotOutput("hist")
)
server <- function(input, output) {
output$hist <- renderPlot({
hist(rnorm(input$num))
})
}
shinyApp(ui = ui, server = server)
library(shiny)
ui <- fluidPage(
sliderInput(inputId = "num",
label = "Choose a number",
value = 25, min = 1, max = 100),
plotOutput("hist")
)
server <- function(input, output) {
output$hist <- renderPlot({
title="100 Normal Random Values"
hist(rnorm(input$num))
})
}
shinyApp(ui = ui, server = server)
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
"#FF7F00", "#FFFF33", "#A65628", "#F781BF", "#999999"))
library(shiny)
ui <- fluidPage(
headerPanel('Iris k-means clustering'),
sidebarPanel(
selectInput('xcol', 'X Variable', names(iris)),
selectInput('ycol', 'Y Variable', names(iris),
selected = names(iris)[[2]]),
numericInput('clusters', 'Cluster count', 3,
min = 1, max = 9)
),
mainPanel(
plotOutput('plot1')
)
)
server <- function(input, output) {
selectedData <- reactive({
iris[, c(input$xcol, input$ycol)]
})
clusters <- reactive({
kmeans(selectedData(), input$clusters)
})
output$plot1 <- renderPlot({
par(mar = c(5.1, 4.1, 0, 1))
plot(selectedData(),
col = clusters()$cluster,
pch = 20, cex = 3)
points(clusters()$centers, pch = 4, cex = 4, lwd = 4)
})
}
shinyApp(ui = ui, server = server)
data(mtcars)
head(mtcars)
str(mtcars)
?mtcars
mod=lm(mpg~factor(am), data=mtcars)
summary(mod)
mod1=lm(mpg~factor(am)-1, data=mtcars)
summary(mod1)
setwd(paste(getwd(), "MachineLearning", sep="/"))
getwd()
temp=tempfile()
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", temp)
temp1=tempfile()
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", temp1)
train<-read.csv(temp, na.strings=c("NA","#DIV/0!", ""))
test=read.csv(temp1, na.strings=c("NA","#DIV/0!", ""))
names(train)[1:15]
suppressMessages(library(caret))
nearZeroVar(train, saveMetrics=FALSE)
index<-c(1:7, 14,17,26, 51:59, 75, 78, 79, 81, 82, 89, 92, 101, 127, 130, 131, 134, 137, 139, 142:150 )
Train<-train[, -index]
naRate<-colSums(is.na(Train))/nrow(Train)
naRate
Index<-which(naRate==0.00)
Train<-Train[, Index]
colSums(is.na(Train))
Test<-test[, -index]
Test<-Test[, Index]
str(Test$problem_id) # making sure the variable is not of value
Test<-Test[, -53] # getting rid of column problem_id
dim(Test)
Train$classe<-as.factor(Train$classe)
inTrain<-createDataPartition(Train$classe, p=0.75, list=FALSE)
training<-Train[inTrain, ]
valid<-Train[-inTrain, ]
data<-training[-53]
newdata<-valid[-53]
set.seed(1379)
suppressMessages(library(foreach))
suppressMessages(library(randomForest))
suppressMessages(library(doParallel))
RF<-foreach(ntree=rep(500,2), .combine=combine, .packages='randomForest') %dopar% randomForest(data, training$classe, ntree=ntree)
RF<-foreach(ntree=rep(500,2), .combine=combine, .packages='randomForest') %dopar% randomForest(data, training$classe, ntree=ntree)
RF
varImpPlot(RF, main="Random Forest model: Important Variables", col="blue", pch=19)
predRF<-predict(RF, newdata)
cmRF<-confusionMatrix(predRF, valid$classe)
cmRF
paste("Model Accuracy:", "", round(cmRF$overall["Accuracy"], 4))
predictions<-predict(RF, Test)
predictions
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictions)
accuracy(predRF, valid$classe)
library(e1071)
accuracy(predRF, valid$classe)
library(forecast)
accuracy(predRF, valid$classe)
summary(RF)
summary(RF)$oob.times
summary(RF)$oob
?createDataPartition
cmRF
print(RF)
?predict
qplot(predRF, classe, data=valid)
pred<-predict(RF, data)
cm<-confusionMatrix(pred, training$classe)
cm
set.seed(357)
inTrain<-createDataPartition(Train$classe, p=0.6, list=FALSE)
training<-Train[inTrain, ]
valid<-Train[-inTrain, ]
data<-training[-53]
newdata<-valid[-53]
set.seed(1379)
suppressMessages(library(foreach))
suppressMessages(library(randomForest))
suppressMessages(library(doParallel))
RF<-foreach(ntree=rep(500,2), .combine=combine, .packages='randomForest') %dopar% randomForest(data, training$classe, ntree=ntree)
RF
varImpPlot(RF, main="Random Forest model: Important Variables", col="blue", pch=19)
pred<-predict(RF, data)
cm<-confusionMatrix(pred, training$classe)
cm
predRF<-predict(RF, newdata)
cmRF<-confusionMatrix(predRF, valid$classe)
cmRF
paste("Model Accuracy:", "", round(cmRF$overall["Accuracy"], 4))
paste("Model Accuracy:", "", round(cmRF$overall["Kappa"], 4))
paste("Model Accuracy:", "", round(cmRF$overall["Accuracy"], 4))
paste("Kappa Statistic:", "", round(cmRF$overall["Kappa"], 4))
predictions<-predict(RF, Test)
predictions
```{r}
